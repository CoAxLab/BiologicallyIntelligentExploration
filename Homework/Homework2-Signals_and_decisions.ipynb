{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cJKaHsOjlWkwvdCZT0zr2klLYATRm57A","timestamp":1663949126924}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/CoAxLab/BiologicallyIntelligentExploration/blob/main/Labs/Lab3_Signal_detection_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"z9tWHz-AZcIy"}},{"cell_type":"markdown","metadata":{"id":"nBFMw9bX1y78"},"source":["# **Homework 2: Signals and decisions**\n","\n","## Getting started\n","\n","This homework will involve concepts from the labs we've gone over in class. Feel free to reference them as you complete the assignment.\n","\n","This homework contains 3 sections:\n","1. Simulating random walks.\n","1. Working with signal detection theory.\n","1. Simulating decision making behavior."]},{"cell_type":"markdown","metadata":{"id":"kjyn1J2MV1lq"},"source":["---\n","## Part 1 - Simple random walk simulations\n","\n","Here you will run a few simulations to see how the different parameters on random walk (Gaussian vs. Levy) influence the behavior of the agents.\n","\n","Fill out the code cells below to complete the assignment.  Most of the programming is extremely straight-forward, as it is all based on the code from the lab, which you can use/modify in this notebook."]},{"cell_type":"markdown","metadata":{"id":"kY8ciO__VlDg"},"source":["### Install _explorationlib_"]},{"cell_type":"code","metadata":{"id":"oZlPnwa-VaK2"},"source":["!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n","!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git\n","!pip install celluloid # for the gifs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rO5gZcxcVxT6"},"source":["### Import modules"]},{"cell_type":"code","metadata":{"id":"vgFy5_M_VwhL"},"source":["# from the standard library\n","import shutil\n","import glob\n","import os\n","import copy\n","import sys\n","\n","# these are common to scientific programming in python\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Computational experiments are run with 'experiment'\n","from explorationlib.run import experiment\n","\n","# Here are some tools to select, save, and load\n","# data from computational experiments\n","from explorationlib.util import select_exp\n","from explorationlib.util import load\n","from explorationlib.util import save\n","\n","# All the explorers we will play with are called\n","# \"agents\"; a bit of computer science jargon\n","from explorationlib.agent import DiffusionDiscrete\n","from explorationlib.agent import GradientDiffusionGrid\n","from explorationlib.agent import GradientDiffusionDiscrete\n","from explorationlib.agent import AccumulatorGradientGrid\n","from explorationlib.agent import TruncatedLevyDiscrete\n","\n","# The environments we will simulate live in a \"gym\"\n","from explorationlib.local_gym import ScentGrid\n","from explorationlib.local_gym import create_grid_scent\n","from explorationlib.local_gym import uniform_targets\n","from explorationlib.local_gym import constant_values\n","\n","\n","# A bunch of tools for plotting and for\n","# movie making\n","from explorationlib.plot import plot_position2d\n","from explorationlib.plot import plot_length_hist\n","from explorationlib.plot import plot_length\n","from explorationlib.plot import plot_targets2d\n","from explorationlib.plot import show_gif\n","\n","# A couple metrics for scoring how well, or poorly,\n","# an exploration experiment went.\n","from explorationlib.score import search_efficiency\n","from explorationlib.score import total_reward\n","from explorationlib.score import first_reward\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErkmvoTlWO70"},"source":["#### Additional notebook configuration"]},{"cell_type":"code","metadata":{"id":"21y0XOi-WYpU"},"source":["\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","%config IPCompleter.greedy=True\n","\n","plt.rcParams[\"axes.facecolor\"] = \"white\"\n","plt.rcParams[\"figure.facecolor\"] = \"white\"\n","plt.rcParams[\"font.size\"] = \"16\"\n","%config IPCompleter.greedy=True\n","%load_ext autoreload\n","%autoreload 2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1-OfpCLahk6"},"source":["## Task - Creating agents agents [10 pt]\n","\n","Run a batch of simulations with the following agents:\n","\n","- DiffusionDiscrete\n","- TruncatedLevyDiscrete\n","- GradientDiffusionDiscrete\n","\n","The last agent is a simple agent that takes input to make the decisions. It uses a gradient search as discussed in Lab 4, but without using evidence accumulation. If it detects a scent gradient it moves up it until the gradient decreases, then it moves again.\n","\n","So for this we will need to set up a sense environment, which we call the _ScentGrid_.\n","\n","The code for running these simulations will be very similar to the simulation code in the lab. We will scaffold some of it for you here."]},{"cell_type":"markdown","metadata":{"id":"B0hCfWgPa29o"},"source":["### Initialize and run the experiments\n","\n","- set the parameters\n","- create the environment\n","- create the agents\n","- run the experiments\n","\n","Feel free to create as many code cells as you would like."]},{"cell_type":"code","metadata":{"id":"RewATQN1WZvu"},"source":["# Experiment settings\n","num_experiments = 10\n","num_steps = 1000\n","p_neg = 1\n","p_pos = 0.5\n","scent_sigma = 10\n","\n","# Env\n","detection_radius = 1\n","min_length = 1\n","max_length = 10\n","\n","env = ScentGrid(mode=\"discrete\")\n","boundary = (100, 100)\n","target = (5,5)\n","coord, scent = create_grid_scent(boundary, amplitude=1, sigma=scent_sigma)\n","env.add_scent(target, 1, coord, scent)\n","\n","# Agents\n","diff = DiffusionDiscrete(min_length=min_length, scale=1)\n","levy = TruncatedLevyDiscrete(min_length=min_length, max_length=max_length, exponent=2)\n","sniff = GradientDiffusionDiscrete(num_actions=4, min_length=min_length, scale=2, p_neg=p_neg, p_pos=p_pos)\n","\n","# Cleanup \n","for path in glob.glob(\"data/test4_*.pkl\"):\n","    os.remove(path)\n","\n","# Run Sims\n","# Write your own agent run code here following the exampel from Labs 2 & 4\n","\n","### your code below\n","levy_exp = experiment(\n","    ## your code here\n",")\n","diff_exp = experiment(\n","    ## your code here\n",")\n","sniff_exp = experiment(\n","    ## your code here\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOunUmnqbuv-"},"source":["### Visualize the trajectories [10 pt]\n","\n","Plot the trajectories taken by the agents in all 10 experiments. Use blue to plot the levy agent, brown for the diffusion agent, and green for the sniff agent. Follow examples from Labs 2 & 4."]},{"cell_type":"code","metadata":{"id":"A2o8Fuw5b0Dt"},"source":["# View size\n","plot_boundary = (20, 20)\n","\n","# Generate 10 plots of walking\n","### your code below\n","for n in range(num_experiments):\n","    ax = None\n","    ax = plot_position2d(\n","        ## your code here\n","    )\n","    ax = plot_position2d(\n","        ## your code here\n","    )\n","\n","    ax = plot_position2d(\n","        ## your code here\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the average distances traveled for all three agents just on the first experiment (indexed at 0)."],"metadata":{"id":"7ADBvqXXXmO3"}},{"cell_type":"code","source":["num_experiment = 0\n","ax = None\n","ax = plot_length_hist(\n","    select_exp(levy_exp, num_experiment),\n","    loglog=False,\n","    bins=50,\n","    density=True,\n","    alpha=0.6,\n","    label=\"Levy\",\n","    color=\"blue\",\n","    ax=ax,\n","    figsize=(6,4),\n",")\n","ax = plot_length_hist(\n","    select_exp(diff_exp, num_experiment),\n","    loglog=False,\n","    bins=50,\n","    density=True,\n","    alpha=0.6,\n","    color=\"brown\",\n","    label=\"Diffusion\",\n","    ax=ax,\n",")\n","\n","ax = plot_length_hist(\n","    select_exp(sniff_exp, num_experiment),\n","    loglog=False,\n","    bins=50,\n","    density=True,\n","    alpha=0.6,\n","    color=\"green\",\n","    label=\"Sniff\",\n","    ax=ax,\n",")\n","sns.despine() # Make pretty plot"],"metadata":{"id":"bR69k1nfZAL0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Summarize the performance [10 pt]\n","\n","Next plot the average scores for each agent. See plotting in Lab 2 for help."],"metadata":{"id":"MjNSPANFZn0j"}},{"cell_type":"code","source":["# Results, names, and colors\n","results = [levy_exp, diff_exp, sniff_exp]\n","names = [\"Levy\", \"Diffusion\", \"Sniff\"]\n","colors = [\"blue\", \"brown\", \"green\"]\n","\n","# Score by efficiency\n","scores = []\n","### your loop code to create score variable\n","\n","# Tabulate means and standard deviations\n","m, sd = [], []\n","# m is mean\n","# sd is standard deviation\n","### your loop code here\n","\n","\n","# Plot\n","fig = plt.figure(figsize=(3, 3))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Score\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"7D5If-2kXrAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88Wenamub4h_"},"source":["### Question 1.1 [5 pt]\n","\n","Do you notice any tangible differences between the two random agents and the agent that uses sensory information?"]},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"UPrz9cZmNuZ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JSEHudnYb1jv"},"source":["## Task - Decreasing Scent Radius\n","\n","Rerun the code in the previous section, but changing the scent radius (how far scent diffuses from each target) of the environment from 10 to 1. "]},{"cell_type":"markdown","source":["### Question 1.2 [5 pt]\n","\n","How does the performance of the agents change?"],"metadata":{"id":"pLkJjchwbygV"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"lQfDlNwbb5V-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2Ot2hHUchtv"},"source":["### Question 1.3 [5 pt]\n","\n","Speculate why the reduction in the scent radius caused the change reported in Question 1.2. "]},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"VtPt9ByYN5zZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Part 2 - Signal Detection Theory\n","\n","Now we are going to pivot to working on playing with signal detection theory. Review Lab 3 if you need help."],"metadata":{"id":"1XdPMVe7yGd_"}},{"cell_type":"markdown","source":["### Section - Setup"],"metadata":{"id":"Gvp_2J-2uwzu"}},{"cell_type":"code","source":["# Import ADMCode library\n","# ADMCode uses an old version of numba\n","!pip install numba==0.48\n","!pip install --upgrade git+https://github.com/CoAxLab/AdaptiveDecisionMaking_2018"],"metadata":{"id":"i3HG9Igu0sJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import modules\n","from __future__ import division\n","from ADMCode import visualize as vis\n","from ADMCode import ddm, sdt\n","\n","import numpy as np\n","import pandas as pd\n","\n","from ipywidgets import interactive\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","warnings.simplefilter('ignore', np.RankWarning)\n","warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n","warnings.filterwarnings(\"ignore\")\n","sns.set(style='white', font_scale=1.3)\n","\n","%matplotlib inline"],"metadata":{"id":"5IZzCMWO0woW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task - Calculate $d'$ and $c$ directly [10 pt]\n","\n","In the code cell below, calculate and print the $d'$ and $c$ values that we would expect from a *hit rate* of 0.85 and *false alarm* rate of 0.30, given the assumptions of SDT."],"metadata":{"id":"j1ZZP2RDcn0K"}},{"cell_type":"code","source":["from scipy.stats import norm # import statistical library for inverse norms\n","\n","# set your hit and FA rates to calculate resulting d' and c values\n","p_hit = ## your code here\n","p_FA  = ## your code here\n","\n","# calculate and print d' value\n","d_prime = ## your code here\n","\n","print(\"d' = {}\".format(d_prime))\n","\n","# calculate and print c value\n","c = ## your code here\n","\n","print(\"c  = {}\".format(c))"],"metadata":{"id":"gE-lpxQSvPU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.1 [5 pt]\n","\n","What are the values for $d'$ and $c$?  "],"metadata":{"id":"5iFvgQxVCurb"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"N9iF6q5MqZjp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task - Playing with signal-to-noise distributions.\n","\n","Now let us run the widget code and play with some parameters to answer the questions below."],"metadata":{"id":"P9C0QHHxvVoH"}},{"cell_type":"code","source":["# run this code cell to load the SDT widget\n","interactive_plot = interactive(vis.sdt_interact, pH=(0.,1.,.1), pFA=(0.,1.,.1))\n","output = interactive_plot.children[-1]\n","output.layout.height = '300px'\n","interactive_plot"],"metadata":{"id":"s7QHzYgLziwU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.2 [5 pt]\n","\n","Describe the relationship between the **false alarm rate** and $d'$ in SDT (use the interactive visualization to help get some intuition), if you hold **hit rate** at a constant value."],"metadata":{"id":"AWbwNFTNxeFn"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"ThOa8karxoE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.3 [5 pt]\n","\n","Describe the relationship between number of **correct rejections** (1 - **false alarms**) and the criterion parameter ($c$)."],"metadata":{"id":"I2j0oHiZxh2U"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"eB_mY7xVxosI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.4 [5 pt]\n","\n","Describe in plain words why when the **hit** and **false alarm** rates are equal, $d'$ is zero but $c$ isn't necessarily zero."],"metadata":{"id":"HFPEFo1wxlJw"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"4UWxbNv1xpg3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Part 3 - Evidence Accumulation\n","\n","This part has two main sections:\n","\n","- First, we will build off the lab by exploring the relationship between decision threshold (one of the DDM parameters) and the performance of an accumulating agent.\n","- Second, we will take a brief look at the role of randomness in the exploratory behavior of a sniffing agent."],"metadata":{"id":"lWuhozWc7A4M"}},{"cell_type":"markdown","source":["### Install necessary import explorationlib modules"],"metadata":{"id":"VEfT0DrIlnnu"}},{"cell_type":"markdown","source":["#### Import modules"],"metadata":{"id":"Dtah7i_l-Agn"}},{"cell_type":"code","source":["# Agents\n","from explorationlib.agent import DiffusionDiscrete\n","from explorationlib.agent import GradientDiffusionDiscrete\n","from explorationlib.agent import AccumulatorGradientGrid\n","from explorationlib.agent import TruncatedLevyDiscrete\n","\n","# Env\n","from explorationlib.local_gym import ScentGrid\n","from explorationlib.local_gym import create_grid_scent\n","\n","# Score\n","from explorationlib.score import total_reward\n","from explorationlib.score import num_death"],"metadata":{"id":"SSeoMBypAD1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task - Influence of Threshold on Performance\n","\n","How does decision thershold influence behavior? Here, we will perform a parameter sweep over the decision threshold to see how the agent is affected in a basic sniffing task.\n","\n","For this section, you will need to slightly modify the lab code to loop over a list of thresholds rather than a list of drift rates."],"metadata":{"id":"WRfyFh7r-KW_"}},{"cell_type":"markdown","source":["### Define shared parameters and initialize environment"],"metadata":{"id":"7--2JPZV-aJf"}},{"cell_type":"code","source":["# Shared exp parameters\n","num_steps = 200\n","max_steps = 10\n","seed_value = 5838\n","\n","min_length = 1\n","step_size = 0.1\n","\n","noise_sigma = 2\n","detection_radius = 1\n","num_targets = 250 \n","target_boundary = (100, 100)\n","\n","# Env\n","env = ScentGrid(mode=None)\n","env.seed(seed_value)\n","\n","# Targets\n","prng = np.random.RandomState(seed_value)\n","targets = uniform_targets(num_targets, target_boundary, prng=prng)\n","values = constant_values(targets, 1)\n","\n","# Scents\n","coord, scent = create_grid_scent(target_boundary, amplitude=1, sigma=10)\n","scents = [scent for _ in range(len(targets))]\n","env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"],"metadata":{"id":"Wqh-TYwa-c4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a list of thresholds to test\n","\n","We want to take our sniff agent with evidence accumulation, and loop through a set of 5 threshold values: [1, 2, 3, 4, 5]. "],"metadata":{"id":"rsWZxdlY-hj8"}},{"cell_type":"code","source":["# Our parameters \n","thresholds = [1.,2.,3.,4.,5.]\n","\n","# For plotting\n","colors = [\"darkgreen\", \"seagreen\", \"cadetblue\", \"steelblue\", \"mediumpurple\"]\n","names = thresholds "],"metadata":{"id":"paWJIcuK-j9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run 100 experiments for each threshold [10 pts]\n","\n","Borrow the loop code from Lab 4. Set the following parameters:\n","\n","- drift_rate = 1.0\n","- accmulation_sigma = 1.0\n","\n","Run 100 experiments across the 5 threshold levels"],"metadata":{"id":"yumLSxtx-kqY"}},{"cell_type":"code","source":["# your code cells below\n","# Exp params\n","drift_rate = 1.0\n","accumulate_sigma = 1.0\n","\n","num_experiments = 100\n","\n","# Run\n","results = []\n","for i, threshold in zip(names, thresholds):\n","    accum = AccumulatorGradientGrid(\n","        ## your code here\n","    )\n","    accum.seed(seed_value)\n"," \n","    exp = experiment(\n","        ## your code here\n","    )\n","    results.append(exp)"],"metadata":{"id":"rm_b3CNA-pDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create plots of distance traveled averaged for each agent (i.e., each threshold)."],"metadata":{"id":"JsuligiyA80W"}},{"cell_type":"code","source":["# your code cells here\n","# Score\n","scores = []\n","for result in results:  \n","    l = 0.0\n","    for r in result:\n","        l += r[\"agent_total_l\"][-1]\n","    scores.append(l)   \n","\n","# Tabulate\n","m, sd = [], []\n","for s in scores:\n","    m.append(np.mean(s))\n","\n","# -\n","fig = plt.figure(figsize=(3, 3))\n","plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total distance\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"fDkmaSK9A_4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the average number of deaths for each agent (i.e., threshold)."],"metadata":{"id":"h_9oPvlevv7j"}},{"cell_type":"code","source":["# Score\n","scores = []\n","for result in results:\n","    scores.append(num_death(result))   \n","\n","# -\n","fig = plt.figure(figsize=(3, 3))\n","plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Deaths\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"T5Q-vZKxBDm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the score of the best agent for each type of agent (i.e., threshold)"],"metadata":{"id":"D3xkDBFbv8Hj"}},{"cell_type":"code","source":["# Score\n","scores = []\n","for result in results:\n","    r = total_reward(result)\n","    scores.append(r)   \n","\n","# Tabulate\n","m = []\n","for s in scores:\n","    m.append(np.max(s))\n","\n","# -\n","fig = plt.figure(figsize=(3, 3))\n","plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Best score\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"3SmZBrk3BGeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finally plot the average score for each type of agent (i.e., threshold)."],"metadata":{"id":"_Smn5dhYwHfU"}},{"cell_type":"code","source":["# Score\n","scores = []\n","for result in results:  \n","    r = total_reward(result)\n","    scores.append(r)   \n","\n","# Tabulate\n","m, sd = [], []\n","for s in scores:\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(6, 3))\n","plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Avg. score\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists of means\n","fig = plt.figure(figsize=(6, 3))\n","for (i, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n","    plt.legend()\n","    plt.xlabel(\"Score\")\n","    plt.tight_layout()\n","    sns.despine()"],"metadata":{"id":"gGA2SjKVBJr3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 3.1 [5 pt]\n","\n","Based on the plots above and in your own words, summarize the relationship between the decision _threshold_ and the performance of the agnets.  How do the effects of increasing threshold compare to the effects of increasing drift rate (from the example in Lab 4)?"],"metadata":{"id":"imEL5IluBN-V"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"dhJzu-YsCUVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 3.2 [5 pt]\n","\n","Based on your observations, do you think an accumulator is needed for this *particular* task setup?  Would you expect an agent which doesn't accumulate (having a threshold of almost zero, thus reacting immediately to its first sensory signal) to outform an agent which uses multiple time steps to accumulate evidence?"],"metadata":{"id":"RsiACjIBBXim"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"618NRuXpCP9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 3.3 [5 pt]\n","\n","Let's get philosophical. In what scenarios would pure random exploration (e.g., the diffusion and levy agents) be better than agents that utilize the energy necessary to process and accumulate sensory signals (e.g., the sniff agents)? \n"],"metadata":{"id":"GKc315DwBhmM"}},{"cell_type":"code","source":["# Write your answer here, as a python comment\n"],"metadata":{"id":"wAhM5dWbCOIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLE6JH1Q1y8R"},"source":["**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here. \n","> *Write Name(s) here*"]},{"cell_type":"markdown","metadata":{"id":"PNv3APHl1y8R"},"source":["**DUE:** 5pm ET, Oct. 7, 2022. Email the link to the completed notebook on your Github repository to the TA and me via Canvas."]}]}