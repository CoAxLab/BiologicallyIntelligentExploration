{"cells":[{"cell_type":"markdown","metadata":{"id":"uV4dbZKi7xPW"},"source":["# **Homework 4: Foraging and reinforcement learning**\n","\n","## Getting started\n","\n","This homework will involve concepts from the labs we've gone over in class. Feel free to reference them as you complete the assignment.\n","\n","This homework contains 2 sections:\n","1. Investigation of patchy environment random initialization effects on foraging agents with different strategies.\n","1. Investigation of various actor-critic agents in a new type of dynamic bandit task - one where one arm becomes *more* rewarding partway through each experiment.\n","\n","Fill out the code cells below and answer the questions to complete the assignment. Most of the programming is quite straightforward, as it is all based on code from the labs, which you can use/modify in this notebook."]},{"cell_type":"markdown","metadata":{"id":"GPqOQU-Y8Ht_"},"source":["---\n","## Section 1 - Foraging [57 pt]\n","\n","In Lab 7, you investigated how random search, chemotaxis, and infotaxis agents behaved in a \"patchy\" foraging environment. We didn't get to testing out the effects of random initializations in class. In this section of the homework you will carry out that analysis.\n","\n","Following the environment patches as bushes metaphor, different random seeds determine where the random bushes grow."]},{"cell_type":"markdown","source":["### Question 1.1 [6 pt]\n","Why is it important to check multiple random seeds when comparing foraging strategies in patchy environments?"],"metadata":{"id":"bqumqsOnuCvy"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"XaEQcgdGu9Ul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### In the code cells below, run and fill in code as needed according to the text instructions before each one. Feel free to refer to lab 7 for help."],"metadata":{"id":"TFnOeuRdtzf7"}},{"cell_type":"markdown","metadata":{"id":"x6c_FVu6GMlW"},"source":["Change the directory to where we want to clone in the specific explorationlib code library branch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"832T18Sd7JKm"},"outputs":[],"source":["cd /content"]},{"cell_type":"markdown","metadata":{"id":"5RsheF0WGPvI"},"source":["Clone in the `target-patch-dev` explorationlib branch (the branch that has our new patchy environment functions)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Y0o__cZGDE2"},"outputs":[],"source":["!git clone -b target-patch-dev https://github.com/coaxlab/explorationlib"]},{"cell_type":"markdown","metadata":{"id":"9Zt7iqViGU7M"},"source":["Install some other supporting code libraries, like gym-maze, which some explorationlib simulated environment code relies on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTUEgJfhGShF"},"outputs":[],"source":["cd /content/explorationlib"]},{"cell_type":"markdown","metadata":{"id":"Ne2KXaZGGdqn"},"source":["Install some other supporting code libraries, like gym-maze, which some explorationlib simulated environment code relies on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hu2CIlO5GbJK"},"outputs":[],"source":["!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git\n","!pip install celluloid # for the gifs"]},{"cell_type":"markdown","metadata":{"id":"-zd4nqxJGhJf"},"source":["Import specific modules from the libraries we loaded. We'll use these modules to create and plot enviornments, run experiments with different exploration agents in these environments, visualize their behaviors, and evaluate their performance according to various metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qk0_KTjiGj6X"},"outputs":[],"source":["# Import misc\n","import shutil\n","import glob\n","import os\n","import copy\n","import sys\n","\n","# Vis - 1\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Exp\n","from explorationlib.run import experiment\n","from explorationlib.util import select_exp\n","from explorationlib.util import load\n","from explorationlib.util import save\n","\n","# Agents\n","from explorationlib.agent import DiffusionGrid\n","from explorationlib.agent import DiffusionDiscrete\n","from explorationlib.agent import GradientDiffusionGrid\n","from explorationlib.agent import GradientDiffusionDiscrete\n","from explorationlib.agent import AccumulatorGradientGrid\n","from explorationlib.agent import AccumulatorInfoGrid\n","from explorationlib.agent import TruncatedLevyDiscrete\n","\n","# Env\n","from explorationlib.local_gym import ScentGrid\n","from explorationlib.local_gym import create_grid_scent\n","from explorationlib.local_gym import create_grid_scent_patches\n","from explorationlib.local_gym import uniform_targets\n","from explorationlib.local_gym import uniform_patch_targets\n","from explorationlib.local_gym import constant_values\n","\n","# Vis - 2\n","from explorationlib.plot import plot_position2d\n","from explorationlib.plot import plot_length_hist\n","from explorationlib.plot import plot_length\n","from explorationlib.plot import plot_targets2d\n","from explorationlib.plot import plot_scent_grid\n","\n","# Score\n","from explorationlib.score import total_reward\n","from explorationlib.score import num_death\n","from explorationlib.score import on_off_patch_time"]},{"cell_type":"markdown","metadata":{"id":"piCCwHdRGp7u"},"source":["### Create a new patchy environment [5 pt]\n","\n","In the code block below, set up a new patch environment like our foraging lab in the following way:\n","- Have there be 4 patches of 15 targets each.\n","- Have each patch have radius 3.\n","- Set the random seed to 1257."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcRU0dsUhbnw"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Visualize the patchy environment [3 pt]\n","In the code cell below, make a plot of the patchy environment you just made."],"metadata":{"id":"LXXfnosM2jbu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BN0EyCOqhf4z"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Create the agents [3 pt]\n","In the code cell below, create a random search, chemotaxis, and infotaxis agent like we did in Lab 7."],"metadata":{"id":"QyS0KaUn1NKM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyJImaPHhy_g"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Run the experiments [5 pt]\n","In the code cell below, run 50 experiments of 400 steps each for each of the agents. Note - you may have set the number of experiments and steps earlier during your environment setup code."],"metadata":{"id":"vJ-fZZ3k3a2E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DnBd8fbhiaR"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Visualize proportion of time spent on patches [4 pt]\n","In the code cell below:\n","- Plot bar plots with error bars for the proportion of time spent on patches for each agent.\n","- Plot a histogram for the proportion of time spent on patches for each agent."],"metadata":{"id":"fHDrp_Xi4_wx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLF8U143hk0g"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Visualize total reward [4 pt]\n","In the code cell below:\n","- Plot bar plots with error bars for the total reward for each agent.\n","- Plot a histogram of total reward for each agent."],"metadata":{"id":"G8dO7tir7Omr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYwGIa7ihnIp"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Visualize agent deaths [4 pt]\n","In the code cell below, plot a bar plot of the number of deaths for each agent type."],"metadata":{"id":"E28YZHEY7a0v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxffxLQmhr3R"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Question 1.2 [7 pt]\n","Describe the performance of each agent type according to each of the metrics (on-patch proportion, total reward, deaths). Why do you think this pattern of performance occured?"],"metadata":{"id":"laoh7qR07wOe"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"jSkK71Et8x4B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1.3.1 [8 pt]\n","Re-run your simulations above, but change the seed value for the random number generator. Do this four different times, once each with the following values: 2257, 3257, 4257, 5257. \n","\n","What do you see in each performance metric of the agents with each new seed value (which specifies different unique environments)?"],"metadata":{"id":"lA4Uui7n06jW"}},{"cell_type":"code","source":["# Write your answers here, as Python comments.\n","\n","# --For seed 2257:--\n","\n","\n","# --For seed 3257:--\n","\n","\n","# --For seed 4257:--\n","\n","\n","# --For seed 5257:--\n"],"metadata":{"id":"I-zlGVJC0-qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1.3.2 [8 pt]\n","What does this (your results recorded in Question 1.3.1) tell you about the the difference between the Info and Chemo agents in particular in environments of this type."],"metadata":{"id":"B6KO0J3G_XyV"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"5QT1IEv9_mDm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wiIQseEg8LJW"},"source":["---\n","## Section 2 - Reinforcement learning [43 pt]\n","\n","In the last part of lab 9, you investigated the performance of different reinforcement learnign agents in a changing bandit task, where an arm that used to give the most reward suddenly dropped in reward probability.\n","\n","In this section of the homework, you will build and test reinforcement learning agents in a different changing bandit task - one where an arm that gave zero reward for most of the experiment changes to being rewarding at a very high probability near the end of each experiment."]},{"cell_type":"markdown","source":["Import necessary modules"],"metadata":{"id":"defNbnP6APPO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUngjoktdsge"},"outputs":[],"source":["import shutil\n","import glob\n","import os\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import explorationlib\n","\n","from explorationlib.local_gym import BanditUniform4\n","from explorationlib.local_gym import BanditChange4\n","from explorationlib.agent import BanditActorCritic\n","from explorationlib.agent import Critic\n","from explorationlib.agent import CriticUCB\n","from explorationlib.agent import CriticNovelty\n","from explorationlib.agent import EpsilonActor\n","from explorationlib.agent import RandomActor\n","from explorationlib.agent import SequentialActor\n","from explorationlib.agent import SoftmaxActor\n","from explorationlib.agent import BoundedRandomActor\n","from explorationlib.agent import BoundedSequentialActor\n","from explorationlib.agent import DeterministicActor\n","\n","from explorationlib.run import experiment\n","from explorationlib.score import total_reward\n","from explorationlib.score import action_entropy\n","from explorationlib.util import select_exp\n","from explorationlib.util import load\n","from explorationlib.util import save\n","\n","from explorationlib.plot import plot_bandit\n","from explorationlib.plot import plot_bandit_actions\n","from explorationlib.plot import plot_bandit_critic\n","from explorationlib.plot import plot_bandit_hist"]},{"cell_type":"markdown","source":["Set up for pretty plots"],"metadata":{"id":"pZJY9K7oAahZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0XbX-Vtdzv_"},"outputs":[],"source":["# Pretty plots\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","%config IPCompleter.greedy=True\n","plt.rcParams[\"axes.facecolor\"] = \"white\"\n","plt.rcParams[\"figure.facecolor\"] = \"white\"\n","plt.rcParams[\"font.size\"] = \"16\"\n","\n","# Dev\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["Plotting the structure of the new bandit task before and after the change"],"metadata":{"id":"IOyb9KcJA0C_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"undTN-_Zd7pf"},"outputs":[],"source":["# Shared env params\n","seed = 5030\n","\n","# plot env before\n","env1 = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.0)\n","env1.seed(seed)\n","plot_bandit(env1, alpha=0.6)\n","\n","# plot env after\n","env2 = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.9)\n","env2.seed(seed)\n","plot_bandit(env2, alpha=0.6)"]},{"cell_type":"markdown","source":["### Create this new changing bandit environment [6 pt]\n","To make the environment described above, set up a BanditChange4 environment with the following parameters:\n","- Have the number of trials before the change be 150.\n","- Have minimum and maximim probability of reward set to 0.1 and 0.3, respectively.\n","- Have the probability of reward for the \"best\" arm actually set to 0.0.\n","- Have the probability of reward for that arm after the change set to 0.9.\n","- Set the environment's seed to 5030."],"metadata":{"id":"kdqBpNpyBXyN"}},{"cell_type":"code","source":["# Your code here"],"metadata":{"id":"MuLch54SA_8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.1 [7 pt]\n","When testing later on, we will have each experiment last for 175 steps. What makes this a tricky problem? What would an agent have to do to succeed in this task?"],"metadata":{"id":"Dx6cMoS_Fadh"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"J0_j0pQWGD5b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the reinforcement learning agents [4 pt]\n","\n","In the code cell below, fill in the code for creating each agent. Use the settings from the lab (repeated here for ease):\n","- Random agent: no settings needed\n","- Epsilon-greedy agent: use epsilon value of 0.1\n","- Upper confidence bound agent: use bonus weight of 0.5\n","- Softmax actor critic: use beta value of 7"],"metadata":{"id":"h9Ne5vWcGKsI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV6ShJNUjB9_"},"outputs":[],"source":["ran = BanditActorCritic(\n","    # Fill in random agent code here\n","    \n",")\n","\n","epy = BanditActorCritic(\n","    # Fill in epsilon greedy agent code here\n","    \n",")\n","\n","ucb = BanditActorCritic(\n","    # Fill in upper confidence bound agent code here\n","    \n",")\n","\n","sft = BanditActorCritic(\n","    # Fill in softmax agent code here\n","    \n",")\n","\n","\n","agents = [ran, epy, ucb, sft]\n","names = [\"random\", \"ep-greedy\", \"upper conf. bound\", \"softmax\"]\n","colors = [\"blue\", \"purple\", \"orange\", \"red\"]"]},{"cell_type":"markdown","metadata":{"id":"HWjC9cHbi8iP"},"source":["### Run the experiments [6 pt]\n","\n","Fill in the code cell below to run 500 experiments for each agent, each with 175 steps. Set the seed to 5030 (have a code line for `seed=5030,` after the code line that sets the number of experiments)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79Lb7ZlqjNvq"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Visualize total rewards [4 pt]\n","\n","In the code cell below, add code to plot the total reward for each agent type in the experiements."],"metadata":{"id":"xPlt_qRtHWb4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwCwybcXjQNL"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","source":["### Question 2.2 [8 pt]\n","How did each of the agents do, compared to one another? Why do you think this is the case?"],"metadata":{"id":"ezuUqY06H7J-"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"AfXTih1tIqdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2.3 [8 pt]\n","\n","Re-run just the experiments and reward plotting with the following random seeds: 6030, 7030, 8030, and 9030. Make sure you are just changing the seed for the experiments, not for the bandit task itself.\n","\n","How consistent are the results you see? What does this tell you about the stability of the patterns you described in Question 2.2? Why do you think this is the case?"],"metadata":{"id":"1K_aWauyJn3e"}},{"cell_type":"code","source":["# Write your answer here, as a Python comment. Explain yourself."],"metadata":{"id":"24xsmTwILhyk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here. \n","> *Write Name(s) here*"],"metadata":{"id":"RLMIxgK1bJJx"}},{"cell_type":"markdown","source":["**DUE:** 5pm ET, Dec. 9, 2022. Email the link to the completed notebook on your Github repository to the TA and me via Canvas."],"metadata":{"id":"zNl6EIAjbNB0"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}