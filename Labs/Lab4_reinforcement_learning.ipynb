{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_reinforcement_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcgBIos8mEm"
      },
      "source": [
        "# Reinforcement Learning Lab\n",
        "\n",
        "In this lab we study exploration in the very abstract $n$-armed bandit ask. In this there are $n$ actions to take. Each returns a reward $R$, with some probability $p$. The reward value is either a 1 or a 0. This means the expected value of each arm is simply its probability of reward. Nice and simple right?\n",
        "\n",
        "Our agents are really learning, at last. Reinforcement learning, to be precise. \n",
        "\n",
        "The reward value $Q$ update rule for all agents (below) and arms is the same:\n",
        "\n",
        "$$ Q \\leftarrow Q + \\alpha * (R - Q) $$ [1]\n",
        "\n",
        "Where the learning rate _lr_reward_ is denoted as $\\alpha$, so that the equation above looks nice. The learning rate often gets called $\\alpha$ in the reinforcement learning world. If you are not familiar with the idea of a learning rate, it is what it sounds like. A parameter that controls how much each value update matters. This is, over time, the rate at which learning happens.\n",
        "\n",
        "Remember too that $Q$ is trying to approximate the average reward value of each arm.\n",
        "\n",
        "This kind of difference $(R - Q)$ in Eq [1] is typical of RL. If you're not sure what it means, consider in your head, what would happen to the value update if $Q$ was bigger than the reward $R$ (and overestimate), or if it was smaller. Once you have noodled that a bit, as needed, consider how making $\\alpha$ bigger or smaller might make $Q$ learning faster, or slower, or more or less volatile. (Learning speed and volatility _often_ go together; an annoying matched set.)\n",
        "\n",
        "_Note_: We are not going to use $\\alpha$ here. Just giving you some intuition.\n",
        "\n",
        "We will first be considering three basic agents:\n",
        "- A random agent that picks its actions with a uniform probability for all 4 arms.  Each action is selected independently.\n",
        "- A sequential agent that samples the arms evenly by cycling through its options\n",
        "- An $\\epsilon$-greedy, which picks the \"best\" arm $1-\\epsilon$ of the time, and picks uniformly randomly from the other arms $\\epsilon$ of the time.\n",
        "\n",
        "The first two agents do not have any sense of exploitation, but we can create an exploitative agent by putting a bound on their exploration.  The *bounded* random and *bounded* sequential agents explore for a fixed number of trials (depending on the `bound` we set) according to their respective strategy, and once the bound is exceeded, they will always pick the arm with the highest Q value.\n",
        "\n",
        "The lab has two sections. \n",
        "\n",
        "- _First_, we will explore 4-armed bandits, and get to know our three basic agents.\n",
        "\n",
        "- _Second_, is where things get more interesting. We'll put $\\epsilon$=greedy, and the two bounded agents, in a little competition. \n",
        "\n",
        "Our metric is _total reward_. Maximizing that is the goal of all RL, afterall.\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WViFqUrTvb2A"
      },
      "source": [
        "## Install and import needed modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWN_8Xu8pnj"
      },
      "source": [
        "# Install explorationlib?\n",
        "!pip install --upgrade git+https://github.com/clappm/explorationlib\n",
        "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzMpA1vmEp1a"
      },
      "source": [
        "# import basic modules\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import explorationlib\n",
        "import explorationlib\n",
        "\n",
        "# import the type of environment we will be using\n",
        "from explorationlib.local_gym import BanditUniform4\n",
        "\n",
        "# import the components to build our agents\n",
        "from explorationlib.agent import BanditActorCritic\n",
        "from explorationlib.agent import Critic\n",
        "from explorationlib.agent import EpsilonActor\n",
        "from explorationlib.agent import RandomActor\n",
        "from explorationlib.agent import SequentialActor\n",
        "from explorationlib.agent import BoundedRandomActor\n",
        "from explorationlib.agent import BoundedSequentialActor\n",
        "\n",
        "# import the experimental framework\n",
        "from explorationlib.run import experiment\n",
        "\n",
        "# import some scoring functions\n",
        "from explorationlib.score import total_reward\n",
        "from explorationlib.score import action_entropy\n",
        "\n",
        "# import some utility functions\n",
        "from explorationlib.util import select_exp\n",
        "from explorationlib.util import load\n",
        "from explorationlib.util import save\n",
        "\n",
        "# import some plotting functions\n",
        "from explorationlib.plot import plot_bandit\n",
        "from explorationlib.plot import plot_bandit_actions\n",
        "from explorationlib.plot import plot_bandit_critic\n",
        "from explorationlib.plot import plot_bandit_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDzjQVNErYq"
      },
      "source": [
        "# Pretty plots\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "%config IPCompleter.greedy=True\n",
        "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"font.size\"] = \"16\"\n",
        "\n",
        "# Dev\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCMyIyrRwwm8"
      },
      "source": [
        "## Section 1.1 - The Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kvy5Me2xKZS"
      },
      "source": [
        "Let's look some at the code defining this class of environment.  As you might guess from the import statement, it is found in local_gym.py.\n",
        "\n",
        "*show how to nagivate there in colab*\n",
        "\n",
        "For convience, the class BanditUniform4 (and its parent class, BanditEnv) is copy-pasted below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4MhgPKqxEhQ"
      },
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Basic Bandits\n",
        "# - A base class,\n",
        "# - then several working examples\n",
        "# ---------------------------------------------------------------------------\n",
        "class BanditEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    n-armed bandit environment  \n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    p_dist : list\n",
        "        A list of probabilities of the likelihood that a particular bandit \n",
        "        will pay out\n",
        "    r_dist : list or list or lists\n",
        "        A list of either rewards (if number) or means and standard deviations\n",
        "        (if list) of the payout that bandit has\n",
        "    \"\"\"\n",
        "    def __init__(self, p_dist, r_dist):\n",
        "        if len(p_dist) != len(r_dist):\n",
        "            raise ValueError(\n",
        "                \"Probability and Reward distribution must be the same length\")\n",
        "\n",
        "        if min(p_dist) < 0 or max(p_dist) > 1:\n",
        "            raise ValueError(\"All probabilities must be between 0 and 1\")\n",
        "\n",
        "        for reward in r_dist:\n",
        "            if isinstance(reward, list) and reward[1] <= 0:\n",
        "                raise ValueError(\n",
        "                    \"Standard deviation in rewards must all be greater than 0\")\n",
        "\n",
        "        self.p_dist = p_dist\n",
        "        self.r_dist = r_dist\n",
        "\n",
        "        self.n_bandits = len(p_dist)\n",
        "        self.action_space = spaces.Discrete(self.n_bandits)\n",
        "        self.observation_space = spaces.Discrete(1)\n",
        "        self.seed()\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "        self.state = 0\n",
        "        self.reward = 0\n",
        "        self.done = False\n",
        "\n",
        "        if self.np_random.uniform() < self.p_dist[action]:\n",
        "            if not isinstance(self.r_dist[action], list):\n",
        "                self.reward = self.r_dist[action]\n",
        "            else:\n",
        "                self.reward = self.np_random.normal(self.r_dist[action][0],\n",
        "                                                    self.r_dist[action][1])\n",
        "\n",
        "        return self.state, self.reward, self.done, {}\n",
        "\n",
        "    def last(self):\n",
        "        return self.state, self.reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self.reward = 0\n",
        "        self.done = False\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BanditUniform4(BanditEnv):\n",
        "    \"\"\"A 4 armed bandit.\"\"\"\n",
        "    def __init__(self, p_min=0.1, p_max=0.3, p_best=0.6, best=2):\n",
        "        self.best = [best]\n",
        "        self.num_arms = 4\n",
        "\n",
        "        # ---\n",
        "        self.p_min = p_min\n",
        "        self.p_max = p_max\n",
        "        self.p_best = p_best\n",
        "\n",
        "        # Generate intial p_dist\n",
        "        # (gets overwritten is seed())\n",
        "        p_dist = np.random.uniform(self.p_min, self.p_max,\n",
        "                                   size=self.num_arms).tolist()\n",
        "        p_dist[self.best[0]] = self.p_best\n",
        "\n",
        "        # reward\n",
        "        r_dist = [1] * self.num_arms\n",
        "\n",
        "        # !\n",
        "        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "\n",
        "        # Reset p(R) dist with the seed\n",
        "        self.p_dist = self.np_random.uniform(self.p_min,\n",
        "                                             self.p_max,\n",
        "                                             size=self.num_arms).tolist()\n",
        "        self.p_dist[self.best[0]] = self.p_best\n",
        "\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLqMvTTP8mEr"
      },
      "source": [
        "Let's make a four armed bandit and then plot the expected value of each arm.\n",
        "\n",
        "_Note_: The random seed is fixed. If you change the seed and run the cell below, some of the reward probabilities will change. The probability of the best arm, the optimal value arm is fixed however. It is set to 0.35, and located at arm 2. Try it! Rerun the cell below with different seeds, a few times, to get a sense of how the non-optimal arms can vary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy85DeYt8mEr"
      },
      "source": [
        "# Shared env params\n",
        "seed = 503\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZAb04SI8mEr"
      },
      "source": [
        "## Section 1.2 - Our three agents, unbounded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4xqkYo98mEs"
      },
      "source": [
        "### Our three agents, _unbounded_\n",
        "\n",
        "A word about the code. Our agents this week work in whaat gets called an ActorCritic desgin. This breaks reinforcement learning algorithms into two parts: The Actor does action selection. The Critic estimates the value of each action.\n",
        "\n",
        "Now in normal reinforcement learning, aka not pure exploration, the _Actor_ uses the $Q$ value estimates from the _Critic_ to, in part, make its decisions. Be it explore or exploit. This is indeed the case for the $\\epsilon$-greedy agent, _EpsilonActor_, works. \n",
        "\n",
        "...But... \n",
        "\n",
        "The other two agents--_SequentialActor_ and _RandomActor_--don't explore with value. The are both _max entropy_ action systems, who don't care about reward value or learning _at all_. We have kept the _ActorCritic_ style because it was easy to implement in _explorationlib_. Don't be misled.\n",
        "\n",
        "Below are the class definitions for some of the important classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4fOuGKg1BSR"
      },
      "source": [
        "class BanditActorCritic(BanditAgent):\n",
        "    \"\"\"Actor-Critic agent\"\"\"\n",
        "    def __init__(self, actor, critic, lr_reward=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.lr_reward = float(lr_reward)\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return self.forward(state)\n",
        "\n",
        "    def update(self, state, action, R, next_state, info):\n",
        "        self.critic_R = R_update(action, R, self.critic, self.lr_reward)\n",
        "\n",
        "    def forward(self, state):\n",
        "        action = self.actor(list(self.critic.model.values()))\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.actor.reset()\n",
        "        self.critic.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl2s6LHt1BZt"
      },
      "source": [
        "class RandomActor(BanditAgent):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "        self.num_actions = num_actions\n",
        "        self.actions = list(range(self.num_actions))\n",
        "\n",
        "    def __call__(self, values):\n",
        "        return self.forward(values)\n",
        "\n",
        "    def forward(self, values):\n",
        "        \"\"\"Values are a dummy var. Pick at random\"\"\"\n",
        "        action = self.np_random.choice(self.actions)\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSAhjMZb2gg5"
      },
      "source": [
        "class SequentialActor(BanditAgent):\n",
        "    \"\"\"Choose actions in sequence; ignore value\"\"\"\n",
        "    def __init__(self, num_actions, initial_action=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_actions = int(num_actions)\n",
        "        self.initial_action = int(initial_action)\n",
        "        self.action_count = self.initial_action\n",
        "\n",
        "    def __call__(self, values):\n",
        "        return self.forward(values)\n",
        "\n",
        "    def forward(self, values):\n",
        "        self.action_count += 1\n",
        "        action = self.action_count % self.num_actions\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.action_count = self.initial_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHm1s0_c2jXN"
      },
      "source": [
        "class EpsilonActor(BanditAgent):\n",
        "    def __init__(self, num_actions, epsilon=0.1, decay_tau=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.decay_tau = decay_tau\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def __call__(self, values):\n",
        "        return self.forward(values)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon -= (self.decay_tau * self.epsilon)\n",
        "\n",
        "    def forward(self, values):\n",
        "        # If values are zero, be random.\n",
        "        if np.isclose(np.sum(values), 0):\n",
        "            action = self.np_random.randint(0, self.num_actions, size=1)[0]\n",
        "            return action\n",
        "\n",
        "        # Otherwise, do Ep greedy\n",
        "        if self.np_random.rand() < self.epsilon:\n",
        "            action = self.np_random.randint(0, self.num_actions, size=1)[0]\n",
        "        else:\n",
        "            action = np.argmax(values)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjiZKnyU5ora"
      },
      "source": [
        "The most important code defining the critic, which handles the Q value update rule:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5aV0WQh5ZbC"
      },
      "source": [
        "def R_update(state, R, critic, lr):\n",
        "    \"\"\"TD-ish update\"\"\"\n",
        "    update = lr * (R - critic(state))\n",
        "    critic.update(state, update)\n",
        "\n",
        "    return critic\n",
        "\n",
        "class Critic(BanditAgent):\n",
        "    \"\"\"Template for a Critic agent\"\"\"\n",
        "    def __init__(self, num_inputs, default_value=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.default_value = default_value\n",
        "\n",
        "        self.model = OrderedDict()\n",
        "        for n in range(self.num_inputs):\n",
        "            self.model[n] = self.default_value\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return self.forward(state)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.model[state]\n",
        "\n",
        "    def update(self, state, update):\n",
        "        self.model[state] += update\n",
        "\n",
        "    def replace(self, state, update):\n",
        "        self.model[state] = update\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.model\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.model = state_dict\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = OrderedDict()\n",
        "        for n in range(self.num_inputs):\n",
        "            self.model[n] = self.default_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMFsQGQL3LFZ"
      },
      "source": [
        "Now we can initialize out three basic agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8ZuvV88mEs"
      },
      "source": [
        "ran = BanditActorCritic(\n",
        "    RandomActor(num_actions=env.num_arms),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "seq = BanditActorCritic(\n",
        "    SequentialActor(num_actions=env.num_arms),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "epy = BanditActorCritic(\n",
        "    EpsilonActor(num_actions=env.num_arms, epsilon=0.1),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "\n",
        "# organize them\n",
        "agents = [ran, seq, epy]\n",
        "names = [\"random\", \"sequential\", \"ep-greedy\"]\n",
        "colors = [\"blue\", \"green\", \"purple\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROEgvmQ06df7"
      },
      "source": [
        "## Section 1.3 - Running Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5utZxEOq8mEt"
      },
      "source": [
        "Let's run out our three agents on the environment from before and plot some things about them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWKi9Qz8mEt"
      },
      "source": [
        "num_steps = 12  # Three rounds per arm, about that anyway\n",
        "num_experiments = 1\n",
        "\n",
        "# !\n",
        "results = []\n",
        "for name, agent in zip(names, agents):\n",
        "    log = experiment(\n",
        "        f\"{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=num_experiments,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQUls-Wm8mEt"
      },
      "source": [
        "### Plot action choices \n",
        "with time (aka steps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp0PtTmF8mEu"
      },
      "source": [
        "num_experiment = 0\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    plot_bandit_actions(\n",
        "        select_exp(res, num_experiment), \n",
        "        num_arms=4,\n",
        "        s=4,\n",
        "        title=name, \n",
        "        color=color,\n",
        "        figsize=(2, 1.5)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phiZ20ct8mEu"
      },
      "source": [
        "### Plot histograms of action probability (aka arm choice). \n",
        "\n",
        "_Note_: The flatter these plots are, the closer they are to _maximum entropy_ exploration behavior. I added a measure of the actual entropy to the title of each plot, to make it easier to compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSqkFi6N8mEu"
      },
      "source": [
        "num_experiment = 0\n",
        "ax = None\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    ent = np.round(np.mean(action_entropy(res)), 2)\n",
        "    plot_bandit_hist(\n",
        "        select_exp(res, num_experiment), \n",
        "        bins=list(range(0, 5)),\n",
        "        title=f\"{name} (ent {ent})\", \n",
        "        alpha=0.4,\n",
        "        color=color,\n",
        "        figsize=(3, 3),\n",
        "        ax=ax\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhJLXux-8mEw"
      },
      "source": [
        "## Section 1.4 - Meet our dilemma\n",
        "Should I explore? Should I exploit it!? \n",
        "\n",
        "....I'll flip a weighted coin, \n",
        "\n",
        "...whose weight has a name. It's $\\epsilon$! \n",
        "\n",
        "The smaller $\\epsilon$ is, the less likely the coin flip comes up \"EXPLORE'' and the more likely it comes up on the \"EXPLOIT\" side. If one chooses the exploit side, one is being greedy, right? The bigger $\\epsilon$ the more likely the coin will say \"EXPLORE ''. Etc.\n",
        "\n",
        "Let's play with $\\epsilon$-greedy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8KmM6Df8mEw"
      },
      "source": [
        "num_steps = 4 * 100\n",
        "epsilons = [0.05, 0.5, 0.95]\n",
        "\n",
        "names = [str(epsilon) for epsilon in epsilons]\n",
        "colors = [\"mediumpurple\", \"mediumorchid\", \"mediumvioletred\"]\n",
        "\n",
        "# !\n",
        "results = []\n",
        "for i, (name, epsilon) in enumerate(zip(names, epsilons)):\n",
        "    agent = BanditActorCritic(\n",
        "        EpsilonActor(num_actions=env.num_arms, epsilon=epsilon),\n",
        "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        "    )\n",
        "    log = experiment(\n",
        "        f\"ep_{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=100,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo5fVKL-8mEw"
      },
      "source": [
        "Example behave. Change _num experiment_ to see more examples (0, 99). \n",
        "\n",
        "_Note_: in every experiment we run in this lab, the optimal value arm is _always_ arm 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzifKrLK8mEw"
      },
      "source": [
        "num_experiment = 40\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    plot_bandit_actions(\n",
        "        select_exp(res, num_experiment), \n",
        "        max_steps=200,\n",
        "        s=4,\n",
        "        title=name, \n",
        "        color=color,\n",
        "        figsize=(3, 1)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAsGW64A8mEw"
      },
      "source": [
        "Total reward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1CAs9H38mEw"
      },
      "source": [
        "# Score\n",
        "scores = []\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    r = total_reward(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot means\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPSMJ6v18mEw"
      },
      "source": [
        "fig = plt.figure(figsize=(6, 3))\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    plt.hist(s, label=name, color=c, alpha=0.8, bins=np.linspace(1, 200, 50))\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Total reward\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutovTOd8mEx"
      },
      "source": [
        "Action entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnq-h3Gy8mEx"
      },
      "source": [
        "# Score\n",
        "scores = []\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    r = action_entropy(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot means\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
        "plt.ylabel(\"Entropy\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl27X2ob8mEx"
      },
      "source": [
        "fig = plt.figure(figsize=(6, 3))\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    plt.hist(s, label=name, color=c, alpha=0.8, bins=np.linspace(0, 1.6, 40))\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Entropy\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymuO7Sh8mEx"
      },
      "source": [
        "## Section 2.1 - Bounds and bandits\n",
        "\n",
        "In this section we'll study two pure explorers with bounds. Our friend $\\epsilon$-greedy returns, unchanged.\n",
        "\n",
        "Let's tune the bound parameter of the bounded agents, assuming 400 steps is an ok number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMfNGJVz8mEy"
      },
      "source": [
        "num_steps = 4 * 100\n",
        "num_experiments = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dckgs7zk-zK5"
      },
      "source": [
        "# Defining environment\n",
        "# DO NOT CHANGE:\n",
        "\n",
        "# Shared env params\n",
        "num_experiments = 100\n",
        "seed = 593  \n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG-6VBR98mEz"
      },
      "source": [
        "## Section 2.2 - Tuning the bound.\n",
        "\n",
        "What is the best _bound_ (for this task)?\n",
        "\n",
        "The code below is a first try at tuning _BoundedRandomActor_ and its only parameter _bound_. Run it. Then use the code to try and narrrow down a best choice for this environment. Put each attempt you make in a new cell, as a way to show your work. (Do not change _num steps_ or _num experiments_, above).\n",
        "\n",
        "Note: we are only going to tune the bound for BoundedRandomActor, and we'll recycle this for BoundedSequentialActor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E9mqy-N8mEz"
      },
      "source": [
        "# CHANGE ME\n",
        "start = 4 # min 4\n",
        "stop = num_steps # max num_steps\n",
        "num_search = 10 # ?\n",
        "\n",
        "# -\n",
        "# LEAVE ME\n",
        "bounds = np.linspace(start, stop, num_search).astype(int)\n",
        "names = [str(np.round(bound, 2)) for bound in bounds]\n",
        "\n",
        "# !\n",
        "results = []\n",
        "for i, (name, bound) in enumerate(zip(names, bounds)):\n",
        "    agent = BanditActorCritic(\n",
        "        BoundedRandomActor(num_actions=env.num_arms, bound=bound),\n",
        "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        "    )\n",
        "    log = experiment(\n",
        "        f\"b_{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=num_experiments,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)\n",
        "\n",
        "# Score\n",
        "scores = []\n",
        "for name, res in zip(names, results):\n",
        "    r = total_reward(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s) in zip(names, scores):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.8)\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.xlabel(\"Bound\")\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqRhjwD8mEz"
      },
      "source": [
        "## Section 2.3 - Reporting Results\n",
        "\n",
        "The tuned parameters are entered here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC4l7XG8CH1U"
      },
      "source": [
        "bound = 75\n",
        "epsilon = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPyox4Tc8mEz"
      },
      "source": [
        "# DO NOT CHANGE:\n",
        "# Shared env params\n",
        "seed = 195\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75-yVZxUBrml"
      },
      "source": [
        "bounded_ran = BanditActorCritic(\n",
        "    BoundedRandomActor(num_actions=env.num_arms, bound=bound),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "bounded_seq = BanditActorCritic(\n",
        "    BoundedSequentialActor(num_actions=env.num_arms, bound=bound),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "epy = BanditActorCritic(\n",
        "    EpsilonActor(num_actions=env.num_arms, epsilon=epsilon),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "\n",
        "# organize them\n",
        "agents = [bounded_ran, bounded_seq, epy]\n",
        "names = [\"b-ran\", \"b-seq\", \"ep-greedy\"]\n",
        "colors = [\"blue\", \"green\", \"purple\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMhXX9hTCwg1"
      },
      "source": [
        "num_steps = 400  # Three rounds per arm, about that anyway\n",
        "num_experiments = 250\n",
        "\n",
        "# !\n",
        "results = []\n",
        "for name, agent in zip(names, agents):\n",
        "    log = experiment(\n",
        "        f\"{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=num_experiments,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HLrSY7CSgl"
      },
      "source": [
        "# Score\n",
        "scores = []\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    r = total_reward(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot means\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.xlabel(\"Agent\")\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGM9PYfFD_Jd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}