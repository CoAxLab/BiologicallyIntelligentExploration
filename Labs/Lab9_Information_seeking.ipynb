{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoAxLab/BiologicallyIntelligentExploration/blob/main/Labs/Lab9_Information_seeking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBXA95L-z3Da"
      },
      "source": [
        "# Lab 9 - Information Seeking in Reinforcement Learning\n",
        "\n",
        "This lab has a number components designed to investigate different strategies of information seeking in reinforcement learning, including those beyond $\\epsilon$-greedy.\n",
        "\n",
        "Sections:\n",
        "1. Building a different environment\n",
        "1. Trying out some new, more sophisticated agents, in the new environment\n",
        "1. Testing the agents in a more complex (changing) environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3CfRL_svwB"
      },
      "source": [
        "## Background\n",
        "\n",
        "- In the last lab, we got familiar with basic Q-learning and action selection algorithms (random, sequential, and  $\\epsilon$-greedy).\n",
        "- We focused on the \"exploitation\" (reward maximization) side of the exploration-exploitation dilemma.\n",
        "- However, as we found, exploration is intertwined with exploitation in a complex way (one must explore to estimate which options may give the most reward).\n",
        "- In fact, the question of how to maximize reward in such a formulation is *mathematically intractible*.\n",
        "- Furthermore, the performance of different exploration/exploitation strategies are heavily dependent on environment/task structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmi3UBws-Wt"
      },
      "source": [
        "### Question 0.1\n",
        "What are some limitations/drawbacks of the $\\epsilon$-greedy algorithm? *Hint: think long-term.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3q5Zjo9tKzn"
      },
      "outputs": [],
      "source": [
        "# Write your answer here, as a Python comment. Explain yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZDfwTE1gF7"
      },
      "source": [
        "## Section - Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUirbadVusEG"
      },
      "outputs": [],
      "source": [
        "# Install explorationlib?\n",
        "!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n",
        "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nceu-601kOJ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import explorationlib\n",
        "\n",
        "from explorationlib.local_gym import BanditUniform4\n",
        "from explorationlib.local_gym import BanditChange4\n",
        "from explorationlib.agent import BanditActorCritic\n",
        "from explorationlib.agent import Critic\n",
        "from explorationlib.agent import CriticUCB\n",
        "from explorationlib.agent import CriticNovelty\n",
        "from explorationlib.agent import EpsilonActor\n",
        "from explorationlib.agent import RandomActor\n",
        "from explorationlib.agent import SequentialActor\n",
        "from explorationlib.agent import SoftmaxActor\n",
        "from explorationlib.agent import BoundedRandomActor\n",
        "from explorationlib.agent import BoundedSequentialActor\n",
        "from explorationlib.agent import DeterministicActor\n",
        "\n",
        "from explorationlib.run import experiment\n",
        "from explorationlib.score import total_reward\n",
        "from explorationlib.score import action_entropy\n",
        "from explorationlib.util import select_exp\n",
        "from explorationlib.util import load\n",
        "from explorationlib.util import save\n",
        "\n",
        "from explorationlib.plot import plot_bandit\n",
        "from explorationlib.plot import plot_bandit_actions\n",
        "from explorationlib.plot import plot_bandit_critic\n",
        "from explorationlib.plot import plot_bandit_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41hgDfeW1mQp"
      },
      "outputs": [],
      "source": [
        "# Pretty plots\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "%config IPCompleter.greedy=True\n",
        "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"font.size\"] = \"16\"\n",
        "\n",
        "# Dev\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIQQINrU1r9N"
      },
      "source": [
        "## Section 1 - The environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we make a new environment which is much like the one from the last lab, except the probability of reward from the \"best\" arm is now 0.6 instead of 0.35."
      ],
      "metadata": {
        "id": "LcyZQixlANWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxSPhkXthLHg"
      },
      "outputs": [],
      "source": [
        "# Shared env params\n",
        "num_experiments = 1\n",
        "seed = 5030\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.6)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.1\n",
        "Compared to the bandit task from the last lab, do you think this one is easier? Why?"
      ],
      "metadata": {
        "id": "FFSMV2fEStTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "Z1G-cZQPTBzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_UOmY9d1zKq"
      },
      "source": [
        "## Section 2 - Old & new agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old agents:\n",
        "1. Random actor agent\n",
        "1. Sequential actor agent\n",
        "1. Epsilon-greedy actor agent\n",
        "\n",
        "New agents:\n",
        "3. Upper confidence bound agent\n",
        "1. Novelty critic agent (with epsilon-greedy actor)\n",
        "1. Softmax actor agent\n",
        "\n",
        "#### **Upper confidence bound critic**\n",
        "The upper confindence bound agent takes a measure of uncertainty into account before action selection.\n",
        "- A \"bonus\" is factored into the estimated value of each action before selection.\n",
        "- This bonus takes into account the number of times each action has been selected, giving a boost to the estimated values of actions which have not been tried very often.\n",
        "- The agent then selects the action with the highest combined score value estimate and bonus, *deterministically*.\n",
        "- The combined score for an action $s$ is calculated according to the formula below (where $N_t(a)$ is how many times action $a$ has been tried up to time $t$, and $c$ is the bonus scaling):\n",
        "\n",
        "$$Q_t(a) + c\\sqrt{\\dfrac{\\text{log}(t)}{N_t(a)}}$$\n",
        "\n",
        "#### **Novelty bonus agent**\n",
        "The novelty bonus agent works just like our original epsilon-greedy agent, except it gives itself a boosted reward (actual reward plus `novelty_bonus`) each time it encounters a state that it has never experienced before. Because we are in a bandit environment, this bonus will only occue once.\n",
        "\n",
        "#### **Softmax action selection**\n",
        "Softmax action selction has to do with how to choose an action given Q-value estimates for each action. In other words, it is an alternative to epsilon-greedy.\n",
        "- In softmax action selection, actions are chosen probabilistically.\n",
        "- Actions with greater-Q values are chosen with higher probability than actions with lower Q-values.\n",
        "- A \"temperature\" parameter beta is used to tune the degree to which actions with higher value estimates are chosen over actions with lower value estimates.\n",
        "- With higher temperature, actions are chosen more uniformly. With lower temperature, actions with higher values are chosen much more often in lieu of actions with lower values.\n",
        "- The probability of selecting action $a$ from $K$ possibile actions is calculated according to the formula below, where $\\beta$ is an \"inverse temperature\" parameter (e.g., higher values lead to lower-temperature action selections).\n",
        "\n",
        "$$\\text{P(choosing action $a$)} = \\frac{e^{\\beta Q(a)}}{\\sum_{j=1}^K e^{\\beta Q(a)}} \\ \\ \\ for\\ j=1,2,\\dots,K$$"
      ],
      "metadata": {
        "id": "OiTqbkqTTgMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.1\n",
        "The upper confidence bound agent chooses actions deterministically. How is it still able to explore?"
      ],
      "metadata": {
        "id": "dAg-F3Cv_hWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "aS92iDRh_yUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.2\n",
        "Do you think that the novelty bonus agent will do better or worse than the epsilon-greedy agent (which is almost the same except without the novelty bonus)? Why?"
      ],
      "metadata": {
        "id": "lydWQyIs-mGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "FKGnmU4t_YV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.3\n",
        "Do you think that softmax action selection is better or worse than epsilon-greedy action selection? Why?"
      ],
      "metadata": {
        "id": "S0d_TSlC_4q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "DreJ0BJdAE_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building our agents\n",
        "Here we create the agents using code. *Note the actor-critic strcture. Note where algorithm-dependent parameters are specified.*"
      ],
      "metadata": {
        "id": "CfvYiEaJAm7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH-X5QDPhVsp"
      },
      "outputs": [],
      "source": [
        "ran = BanditActorCritic(\n",
        "    RandomActor(num_actions=env.num_arms),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "seq = BanditActorCritic(\n",
        "    SequentialActor(num_actions=env.num_arms),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "epy = BanditActorCritic(\n",
        "    EpsilonActor(num_actions=env.num_arms, epsilon=0.1),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "ucb = BanditActorCritic(\n",
        "    DeterministicActor(num_actions=env.num_arms),\n",
        "    CriticUCB(num_inputs=env.num_arms, default_value=0.0, bonus_weight=0.5)\n",
        ")\n",
        "nov = BanditActorCritic(\n",
        "    EpsilonActor(num_actions=env.num_arms, epsilon=0.1),\n",
        "    CriticNovelty(num_inputs=env.num_arms, default_value=0.0, novelty_bonus=1.0)\n",
        ")\n",
        "sft = BanditActorCritic(\n",
        "    SoftmaxActor(num_actions=env.num_arms, beta=7),\n",
        "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
        ")\n",
        "\n",
        "\n",
        "# -\n",
        "agents = [ran, seq, epy, ucb, nov, sft]\n",
        "names = [\"random\", \"sequential\", \"ep-greedy\", \"upper conf. bound\", \"novelty\", \"softmax\"]\n",
        "colors = [\"blue\", \"green\", \"purple\", \"orange\", \"cyan\", \"red\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHryLdmV11uK"
      },
      "source": [
        "## Section 3 - Baseline agent behaviors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.1\n",
        "What do you expect the upper confidence bound agent's behavior to look like in our bandit task? Why?"
      ],
      "metadata": {
        "id": "HLByQMMnCEbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "ooLltr3zCY3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.2\n",
        "What do you expect the novelty bonus agent's performance to look like? Why?"
      ],
      "metadata": {
        "id": "xPEyAZwqCdrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "aAc2tCCsCq20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.3\n",
        "What do you expect the softmax agent's performance to look like? Why?"
      ],
      "metadata": {
        "id": "HjQjVMBACsvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "WahLFgXEC6vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's see! Run the experiment and visualization code a few times to see the range of behavior if you're alone, or compare outcomes within a group."
      ],
      "metadata": {
        "id": "PcoevG_MC-Zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxffO_LbhZ97"
      },
      "outputs": [],
      "source": [
        "num_steps = 40\n",
        "\n",
        "# !\n",
        "results = []\n",
        "for name, agent in zip(names, agents):\n",
        "    log = experiment(\n",
        "        f\"{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=num_experiments,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXgp_JDuhdI8"
      },
      "outputs": [],
      "source": [
        "num_experiment = 0\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    plot_bandit_actions(\n",
        "        select_exp(res, num_experiment), \n",
        "        num_arms=4,\n",
        "        s=4,\n",
        "        title=name, \n",
        "        color=color,\n",
        "        figsize=(2, 1.5)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize action choice distributions"
      ],
      "metadata": {
        "id": "N2z33J3lEDcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK6AkByXhf7e"
      },
      "outputs": [],
      "source": [
        "num_experiment = 0\n",
        "ax = None\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    ent = np.round(np.mean(action_entropy(res)), 2)\n",
        "    plot_bandit_hist(\n",
        "        select_exp(res, num_experiment), \n",
        "        bins=list(range(0, 5)),\n",
        "        title=f\"{name} (ent {ent})\", \n",
        "        alpha=0.4,\n",
        "        color=color,\n",
        "        figsize=(3, 3),\n",
        "        ax=ax\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.4\n",
        "Did the new agents' behaviors match your predictions? If not, why do you think the agents behaved as they did?"
      ],
      "metadata": {
        "id": "wU-6fer0DSRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "oQsIYy_2Djj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.5\n",
        "If we collect the results of 100 experiments, which agent do you think will do the best on average? Do you think all our new agents will do better than the \"old\" agents? Why?"
      ],
      "metadata": {
        "id": "VNArxXcoGizK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "StCn0YpXHAdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's see! Run the next two code cells."
      ],
      "metadata": {
        "id": "ZqE70Jz-HBzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the repeated experiments\n",
        "\n",
        "num_steps = 4 * 100\n",
        "\n",
        "results = []\n",
        "for name, agent in zip(names, agents):\n",
        "    log = experiment(\n",
        "        f\"{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=100,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ],
      "metadata": {
        "id": "YCha3fNgFJuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize total rewards\n",
        "\n",
        "# Score\n",
        "scores = []\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    r = total_reward(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot means\n",
        "fig = plt.figure(figsize=(3, 6))\n",
        "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.xlabel(\"Agent\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "Lw56-swcFrRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.6\n",
        "Were your predictions correct? If not, what do you think led to the results you saw?"
      ],
      "metadata": {
        "id": "A00ViiV0HZSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "m1tbHpvwHvWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxtQS9LbV-Z1"
      },
      "source": [
        "## Section 4 - Dynamic environment performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will consider what can happen when the environment isn't static. We'll build an environment where the reward probability of the best arm changes partway through the task.\n",
        "- for this, we will make a `BanditChange4` environment. This is also a bandit task environment with 4 arms. However, there are two extra parameters to specify: `p_change` (the probability of reward that the best arm changes to) and `num_change` (the time point at which the change occurs).\n",
        "- The normal bandit structure plotting code can't handle this type of envionment so there's code below to plot bandits which resemble the task in the two different states."
      ],
      "metadata": {
        "id": "dQkDON92EZTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Nh4k1iikfT"
      },
      "outputs": [],
      "source": [
        "# Shared env params\n",
        "num_experiments = 1\n",
        "seed = 5030\n",
        "\n",
        "# plot env before\n",
        "envA = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.6)\n",
        "envA.seed(seed)\n",
        "plot_bandit(envA, alpha=0.6)\n",
        "\n",
        "# plot env after\n",
        "envB = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.1)\n",
        "envB.seed(seed)\n",
        "plot_bandit(envB, alpha=0.6)\n",
        "\n",
        "# Create changing bandit environment\n",
        "env = BanditChange4(num_change=50, p_min=0.1, p_max=0.3, p_best=0.6, p_change=0.1)\n",
        "env.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4.1\n",
        "Which agent do you think will do the best in this new task? Why?"
      ],
      "metadata": {
        "id": "NRzKP6gCKMsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "XQV2-hK2PaEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the repeated experiments\n",
        "\n",
        "num_steps = 100\n",
        "\n",
        "results = []\n",
        "for name, agent in zip(names, agents):\n",
        "    log = experiment(\n",
        "        f\"{name}\",\n",
        "        agent,\n",
        "        env,\n",
        "        num_steps=num_steps,\n",
        "        num_experiments=100,\n",
        "        dump=False,\n",
        "        split_state=False,\n",
        "    )\n",
        "    results.append(log)"
      ],
      "metadata": {
        "id": "DEN4TCzTKHmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize total rewards\n",
        "\n",
        "# Score\n",
        "scores = []\n",
        "for name, res, color in zip(names, results, colors):\n",
        "    r = total_reward(res)\n",
        "    scores.append(r)   \n",
        "\n",
        "# Tabulate\n",
        "m, sd = [], []\n",
        "for (name, s, c) in zip(names, scores, colors):\n",
        "    m.append(np.mean(s))\n",
        "    sd.append(np.std(s))\n",
        "\n",
        "# Plot means\n",
        "fig = plt.figure(figsize=(3, 6))\n",
        "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.xlabel(\"Agent\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "EQDqV4AdKK2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4.2\n",
        "Were your predictions correct? If not, why do you think this is the case?"
      ],
      "metadata": {
        "id": "qw20uM52PkAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "tNIVX3tHPtqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4.3\n",
        "Given what you've learned today, if you had to choose, which algorithm would you follow for balancing exploration and exploitation in your life?"
      ],
      "metadata": {
        "id": "Vv7GKh0WP6sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here, as a comment. Explain yourself."
      ],
      "metadata": {
        "id": "TuHSKMFtQOZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}