{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework4_reinforcment_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irmvxdQ9Ewkx"
      },
      "source": [
        "# Reinforcment Learning Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VawCitLAE87k"
      },
      "source": [
        "## Section A - Notebook Setup [5 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMFpHjqE9CP"
      },
      "source": [
        "To help prepare you for the class project, we are going to ask you to set up your notebook slightly differently this time: Rather than installing explorationlib from one of its existing git repositories (such as `parenthetical-e` or `clappm`), install it from a git repository that you own.  Fork the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGb5TROIGBzK"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW46l4GRGbTg"
      },
      "source": [
        "## Section B - Understanding the code structure [5 pts]\n",
        "\n",
        "*pop quiz!*\n",
        "\n",
        "In the lab, we went over the python classes for `RandomActor`, `SequentialActor`, and `EpsilonActor`.  We did not however show the code for `BoundedRandomActor` or `BoundedSequentialActor`.  Please post the class definitions for those agents below, so that we know that you know where to find them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPQIDatLHSR9"
      },
      "source": [
        "# BoundedRandomActor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ4E2cemHT_H"
      },
      "source": [
        "# BoundedSequentialActor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_f-nhycH4Hf"
      },
      "source": [
        "## Section C - Understanding Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPos4Zj5IC4b"
      },
      "source": [
        "### Question [10 pts]\n",
        "\n",
        "As discussed in lab, both the Random and Sequential explorers produce max-entropy behavior asymptotically.  If we ran 100 bandit experiments (similar to those in Section 1.3 of Lab), how often do you expect the measured entropy of the Random agent to *exceed* the entropy of the Sequential agent?  Please justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNjLE9LoKeiD"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j81kqBlqQIDK"
      },
      "source": [
        "### Question [5 pts]\n",
        "\n",
        "If you had to pick one (either Random or Sequential) as your personal bandit exploration strategy, which would you choose and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OApMgK14QjpD"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgInxg8Oqj9"
      },
      "source": [
        "### Running sims of the $\\epsilon$-greedy agent [5 pts]\n",
        "\n",
        "Consider the $\\epsilon$-greedy agent we explored in Section 1.4 of lab.  There, we only looked at three values for epsilon (0.05, 0.5, 0.95). Pick at least 3 more episilon values, add them to that list, and rerun the 40 experiments of 400 steps each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2PByP2NOqU1"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5x84IrGQ6K2"
      },
      "source": [
        "### Plotting the results [10 pts]\n",
        "\n",
        "Plot the mean rewards and the mean entropy values for the agents.  (The two bar charts in Section 1.4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQO6ZW6TQ3-m"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnoHUByXMPRe"
      },
      "source": [
        "### Question [5 pts] \n",
        "\n",
        "In your own words, explain how $\\epsilon$ seems to relate to _total reward_ and _action entropy_. As $\\epsilon$ grows, what happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW6oc5sdOIj5"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5pOZLBBNBn5"
      },
      "source": [
        "### Question [5 pts]\n",
        "\n",
        "Let's ask this again from another direction. When looking at the performance of the $\\epsilon$-greedy agents, what relationship do you see between _total reward_ and _action entropy_?  To answer, focus on the the bar plots of entropy and reward.\n",
        "\n",
        "Can you explain this correlation in terms of the explore-exploit dilemma?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KbBPOwQOJbG"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFWpTuTXRkOO"
      },
      "source": [
        "### Question [5 pts]\n",
        "\n",
        "Does there seem to be any relationship between epsilon and the *standard deviation* of rewards (the error bars on the bar chart)?  Why might we expect such a relationship to exist? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQl0HutS47v"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYtZ9eWwTF7O"
      },
      "source": [
        "## Section D - Tuning epsilon [15 pts]\n",
        "\n",
        "In the lab, we did not attempt to tune the value of epsilon.  Now, it is your turn to do so.\n",
        "\n",
        "Try to estimate the optimal value of epsilon (for this very particular bandit task). We recognize that the results will be quite noisy, and that it will be essentially impossible to tell which precise value of $\\epsilon$ is truly the best. Give it a good shot but don't overanalyze it.\n",
        "\n",
        "The coding is up to you, but we ask that you clearly show your tuning process: Put each batch of simulations you run into a separate code cell, so that we can see all of the plots you make. This way, you are showing us your work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfGeZLPGULEU"
      },
      "source": [
        "# DO NOT CHANGE:\n",
        "\n",
        "# Shared env params\n",
        "num_experiments = 100\n",
        "seed = 593  \n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)\n",
        "\n",
        "num_steps = 4 * 100\n",
        "num_experiments = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZCUL498UfWr"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2vtnWqCXJrC"
      },
      "source": [
        "## Section E - Comparing the tuned agents\n",
        "\n",
        "### Coding [10 pts]\n",
        "\n",
        "Rerun the code from Section 2.3 of the lab, to compare the tuned $\\epsilon$-greedy agent to the tuned bounded random and bounded sequential agents. Now, since we have tuned all three agents, the playing field is a bit more level.\n",
        "\n",
        "Plot the average rewards of the three agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHXinhiV2gr"
      },
      "source": [
        "bound = 75  # use 75 as the \"tuned\" bound\n",
        "epsilon = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsiIHVAsX2_5"
      },
      "source": [
        "# your code cell here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6MXF_TZYgtq"
      },
      "source": [
        "### Question [5 pts]\n",
        "\n",
        "So, given these results, what's a better way to maximize total reward in this (reporting) task? Is it BoundedRandomActor? Or, BoundedSeqentialActor? Or is it _EpsilonActor_?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8goveFYs-A"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD4-uVHTY7vj"
      },
      "source": [
        "## Section F - Changing the number of steps\n",
        "\n",
        "Throughout the tuning process, we have assumed that each bandit experiment lasted 400 steps. As a result, our agents are essentially designed to perform well for that particular setup.  In this section, we are going to ask you to run simulation batches with different values of `num_steps`, to see if the results of the previous section still hold true in a more general situation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qb7P8oKZ21G"
      },
      "source": [
        "### Coding task: Re-running Section E with num_steps = 25, 100, and 1600 [10 pts]\n",
        "\n",
        "Between section E and section F combined, you should end up with four total plots, showing the bar chart of mean rewards for step counts 25, 100, 400, and 1600."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2j1vOaaamrA"
      },
      "source": [
        "# your code cell here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi87NehRao1g"
      },
      "source": [
        "### Question [5 pts]\n",
        "\n",
        "Did the same agent always win? Or did different agents perform better at higher or lower step counts (compared to the baseline of 400 steps)?  Base your answer on your plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMRbIvG4bfUt"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nyw5Nk8a8Sq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}