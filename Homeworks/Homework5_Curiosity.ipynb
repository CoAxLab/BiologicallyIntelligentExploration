{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5_Curiosity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkZUzunsm0l2"
      },
      "source": [
        "# \"Curiosity\" Homework\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_szPU-l5eY9"
      },
      "source": [
        "## Section A - \"Choose your fighter\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TXbwTeoyqe"
      },
      "source": [
        "In this homework, we will be exploring (pun intended) some more agents for the multi-armed bandit task. This assignment will also be a bit more open-ended. Up until now, we have always specified which agents to run, but now you will be tasked with \"designing\" your own agents. Not from scratch of course, but from the set of building blocks (Actors and Critics) which we established in Lab 4 and Lab 5.\n",
        "\n",
        "So far we have seen 5 different Actors, which use the estimated values for the different arms to return a selection according to different strategies.\n",
        "\n",
        "- DeterministicActor\n",
        "- BoundedRandomActor (parameterized by bound)\n",
        "- BoundedSequentialActor (parameterized by bound)\n",
        "- EpsilonActor (parameterized by epsilon)\n",
        "- SoftmaxActor (parameterized by beta)\n",
        "\n",
        "We have also seen 3 different Critics, which are responsible for returning the \"value\" of each arm by combining extrinsic and (possibly) intrinsic values.\n",
        "\n",
        "- Critic\n",
        "- CriticUCB\n",
        "- CriticNovelty\n",
        "\n",
        "If you do the math, there's 15 possible combinations. Between labs 4 and 5, we have used 7:\n",
        "\n",
        "- DeterministicActor / Critic\n",
        "- BoundedRandomActor / Critic\n",
        "- BoundedSequentialActor / Critic\n",
        "- EpsilonActor / Critic\n",
        "- SoftmaxActor / Critic\n",
        "- DeterministicActor / CriticUCB\n",
        "- DeterministicActor / CriticNovelty\n",
        "\n",
        "Our main question: which combinations of Actor and Critic work best?\n",
        "\n",
        "To help answer this question, we will ask you to select three new combinations to test out. You will be asked to tune your selected agents and run them in different evironments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfJoXppycXV"
      },
      "source": [
        "### Question 1 [5 pts]\n",
        "\n",
        "Create a list of three *new* Actor/Critic pairings, which will serve as your agents for the remainder of this assignment.\n",
        "\n",
        "There's a second restriction we are adding: you can't use the same critic for all three agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOEC-A_9z9jd"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjFB4Emin5Q3"
      },
      "source": [
        "## Section B - Notebook setup [5 pts]\n",
        "\n",
        "This lab uses the `DeceptiveBanditOneHigh10` environment (along with the `DeceptiveBanditEnv` parent class), which have been newly ported to explorationlib.  Therefore, you will have to update your personal copy of `local_gym.py` to include these two classes (which can be found in the clappm/explorationlib repo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxr_eCk_n5Ti"
      },
      "source": [
        "Install explorationlib, import the agents/critics/environments, and configure the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud65K742mhw5"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AWQg60F0BAs"
      },
      "source": [
        "## Section C - Four-arm bandits [40 pts total]\n",
        "\n",
        "\n",
        "\n",
        "We will first consider the 4-arm environment we've used several times before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NkMInZ43sjG"
      },
      "source": [
        "### Creating the training environment\n",
        "\n",
        "It's always good practice to test your agents on a different environment than the one on which they were trained.  For now we will create a training environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHE__LmN4nBS"
      },
      "source": [
        "# don't touch\n",
        "# Shared env params\n",
        "seed = 412\n",
        "num_steps = 400\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_H3_iUD0PQH"
      },
      "source": [
        "### Question 2 [10 pts]\n",
        "\n",
        "How do you expect your agents to perform relative to each other, in a 4-arm environment after they have been tuned? Which one will do best and which one will do worst?  Please explain your ranking, *considering the functionality and contributions of both the Actor and Critic components of each agent*. To help justify your hypothesis, it will be useful to briefly reference previous simulations and results (Labs 4/5 and HW 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOXz2DXR9_ZB"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv1Xo6Nd3aID"
      },
      "source": [
        "### Tuning Agent 1 for 4-armed bandit [5 pts]\n",
        "\n",
        "Each of your agents should have 1 tunable parameter.  The name and functionality of this parameter depends on which Actors you selected. For this homework, we are not going to be tuning the Critic in any way, we will simply be using the default parameters. The examples from Lab 5 should show you how to create an agent from a combination of Actor and Critic.\n",
        "\n",
        "First tune the parameter of Agent 1, whatever it may be, using the training environment we've established. Show your different simulation batches and plots in different cells so that we may see your work.  The exact process by which you tune your agents is up to you.\n",
        "\n",
        "We understand that tuning can be tedious... we are not asking for perfection. We don't have an answer key for parameter values. The goal is just for you to find parameters that are *good enough* so that the comparison between agents can be considered fair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM148tDW8_tV"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R7lPjsn9GN1"
      },
      "source": [
        "What parameter value did you you settle on for Agent 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BatXXlXu98hS"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-RgdrJe9vmB"
      },
      "source": [
        "### Tuning Agent 2 for 4-armed bandit [5 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XagXrdZT9AaN"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucT5DpCV93bi"
      },
      "source": [
        "What parameter value did you you settle on for Agent 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqzg4rbc96i8"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_dCMc9F-QJ8"
      },
      "source": [
        "### Tuning Agent 3 for 4-armed bandit [5 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2INs6LP-QJ8"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1Q5tCL-QJ8"
      },
      "source": [
        "What parameter value did you you settle on for Agent 3?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C95mJqZ_-QJ9"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzRP_UE5-dSR"
      },
      "source": [
        "### Creating a testing environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qo_Wbq-sBf"
      },
      "source": [
        "# don't touch\n",
        "# Shared env params\n",
        "seed = 15213\n",
        "num_steps = 400\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfio87eG-7vO"
      },
      "source": [
        "### Run 400 experiments and plot the average rewards for the 3 agents [10 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES9VsgeB_Jsw"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MuLE0P4_JGY"
      },
      "source": [
        "### Question 3 [5 pts]\n",
        "\n",
        "Did your results match what you predicted in Question 2? If not, do you have any ideas as to why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_QbWxVs_8tj"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9qJ9oGvACTM"
      },
      "source": [
        "## Section D - Deceptive bandits [50 pts total]\n",
        "\n",
        "We will now consider the same types of agents placed into a different type of environment: the deceptive bandit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrWl5YcACTM"
      },
      "source": [
        "### Creating the training environment\n",
        "\n",
        "We are going to retune our agents, but we're not going to tune them against the deceptive bandit (or else it wouldn't be very deceptive, would it?).  Instead we are going to tune them against the non-deceptive 10-arm bandit from lab, which is identical to the deceptive bandit in terms of arm values but without the deception."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBrPFowLACTM"
      },
      "source": [
        "# don't touch\n",
        "# Shared env params\n",
        "seed = 503\n",
        "num_steps = 400\n",
        "\n",
        "# Create env\n",
        "env = BanditUniform10(p_min=0.2, p_max=0.2, p_best=0.8)\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOzIHeXnB9WU"
      },
      "source": [
        "### Question 4 [10 pts]\n",
        "\n",
        "How well do you expect your agents to perform in the deceptive 10-arm environment, after being tuned in the non-deceptive environment?  Which one will score the highest and which one will score the lowest?\n",
        "\n",
        "Similar to when you made a hypothesis in Section C, explain your answer fully.  Base it on your understanding of the properties of the actors and critics, as well as the results in lab. Consider the possible weaknesses of the agents, such as how certain parameter values might allow the agents to perform better in training but also cause the agents to be more easily deceived."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS3U_jVwB9WU"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FfGrnPsACTN"
      },
      "source": [
        "### Tuning Agent 1 for 10-armed bandit [5 pts]\n",
        "\n",
        "The process for tuning agents here should be roughly the same as in Section C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kumvx_APACTN"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBy-rOxyACTN"
      },
      "source": [
        "What parameter value did you you settle on for Agent 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4VM7jeACTN"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z4ui3BDACTN"
      },
      "source": [
        "### Tuning Agent 2 for 10-armed bandit [5 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qEvday-ACTO"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MerP56ElACTO"
      },
      "source": [
        "What parameter value did you you settle on for Agent 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7bBwY5DACTO"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tI3EhfIACTO"
      },
      "source": [
        "### Tuning Agent 3 for 10-armed bandit [5 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcirzfklACTO"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUWqtT4XACTO"
      },
      "source": [
        "What parameter value did you you settle on for Agent 3?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydy0AtOrACTO"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0R_6EKrACTO"
      },
      "source": [
        "### Creating a testing (deceptive) environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xM2jTxSACTO"
      },
      "source": [
        "# don't touch\n",
        "# Shared env params\n",
        "seed = 15213\n",
        "num_steps = 400\n",
        "\n",
        "# Create env\n",
        "env = DeceptiveBanditOneHigh10()\n",
        "env.seed(seed)\n",
        "\n",
        "# Plot env\n",
        "plot_bandit(env, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV3EG9MQACTO"
      },
      "source": [
        "### Run 400 experiments and plot the average rewards for the 3 agents [10 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZxzgUqACTO"
      },
      "source": [
        "# your code cells here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3PluOMLACTO"
      },
      "source": [
        "### Question 5 [5 pts]\n",
        "\n",
        "Did your results match what you predicted in Question 4? If not, do you have any ideas as to why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUjaT5EzACTO"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-3UNfLZLK3D"
      },
      "source": [
        "### Question 6 [10 pts]\n",
        "\n",
        "Time for some conclusions. Was there a clear winner among your three selected agents?  Was there one which performed the best against both non-deceptive and deceptive bandits?  Or did different agents perform better or worse in different scenarios?  Which agent would you pick as your favorite?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXTxkPUF6Ao6"
      },
      "source": [
        "## Submission\n",
        "\n",
        "**DUE:** 5pm EST, Nov 30, 2021. Email the link to the completed notebook on your Github repository to the TA and me via Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d6uFZOB6kvI"
      },
      "source": [
        "**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here. \n",
        "> *Someone's Name*"
      ]
    }
  ]
}